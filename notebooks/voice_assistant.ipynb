{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a09cfea",
   "metadata": {},
   "source": [
    "# OpenAI Voice Assistant Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7394ad",
   "metadata": {},
   "source": [
    "- Run the installation cell once per environment.\n",
    "- Update the `.env` file with your `OPENAI_API_KEY`.\n",
    "- Each turn: run the last cell, press Enter to start speaking, wait for the response.\n",
    "- Type `r` then Enter to reset conversation memory, `q` to quit the loop.\n",
    "- Ensure your microphone is connected and not muted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205ce742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'simpleaudio' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'simpleaudio'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet openai sounddevice simpleaudio python-dotenv numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09de09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client configured. System prompt loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SYSTEM_PROMPT = os.getenv(\"SYTEM_PROMPT\", \"You are an ai voice assistance\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY in your .env file before proceeding.\")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "print(\"Client configured. System prompt loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102ab647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n(async () => {\n  try {\n    await navigator.mediaDevices.getUserMedia({ audio: true });\n    console.log('Microphone access granted.');\n  } catch (err) {\n    console.error('Microphone access denied:', err);\n  }\n})();\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Javascript, display\n",
    "\n",
    "display(Javascript(\"\"\"\n",
    "(async () => {\n",
    "  try {\n",
    "    await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "    console.log('Microphone access granted.');\n",
    "  } catch (err) {\n",
    "    console.error('Microphone access denied:', err);\n",
    "  }\n",
    "})();\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import threading\n",
    "import time\n",
    "import wave\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import simpleaudio as sa\n",
    "import sounddevice as sd\n",
    "\n",
    "class AudioPlayer:\n",
    "    def __init__(self) -> None:\n",
    "        self._lock = threading.Lock()\n",
    "        self._play_obj: Optional[sa.PlayObject] = None\n",
    "\n",
    "    def play_wav(self, wav_bytes: bytes) -> None:\n",
    "        with self._lock:\n",
    "            if self._play_obj is not None:\n",
    "                self._play_obj.stop()\n",
    "                self._play_obj = None\n",
    "            with wave.open(io.BytesIO(wav_bytes), 'rb') as wf:\n",
    "                frames = wf.readframes(wf.getnframes())\n",
    "                channels = wf.getnchannels()\n",
    "                sample_width = wf.getsampwidth()\n",
    "                sample_rate = wf.getframerate()\n",
    "            self._play_obj = sa.play_buffer(frames, channels, sample_width, sample_rate)\n",
    "\n",
    "    def is_playing(self) -> bool:\n",
    "        with self._lock:\n",
    "            return self._play_obj is not None and self._play_obj.is_playing()\n",
    "\n",
    "    def wait_finish(self) -> None:\n",
    "        \"\"\"Wait for current playback to complete.\"\"\"\n",
    "        with self._lock:\n",
    "            play_obj = self._play_obj\n",
    "        if play_obj is not None:\n",
    "            play_obj.wait_done()\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        with self._lock:\n",
    "            if self._play_obj is not None:\n",
    "                self._play_obj.stop()\n",
    "                self._play_obj = None\n",
    "\n",
    "def record_until_silence(\n",
    "    sample_rate: int = 16000,\n",
    "    threshold: float = 0.01,\n",
    "    silence_duration: float = 1.0,\n",
    "    max_seconds: float = 30.0,\n",
    "    chunk_size: int = 1024\n",
    ") -> Optional[np.ndarray]:\n",
    "    buffer: List[np.ndarray] = []\n",
    "    speaking = False\n",
    "    silence_start: Optional[float] = None\n",
    "    start_time = time.time()\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1, dtype='float32') as stream:\n",
    "        while True:\n",
    "            data, _ = stream.read(chunk_size)\n",
    "            rms = float(np.sqrt(np.mean(np.square(data))))\n",
    "            now = time.time()\n",
    "            if rms > threshold:\n",
    "                if not speaking:\n",
    "                    print('Speech detected...')\n",
    "                speaking = True\n",
    "                silence_start = None\n",
    "                buffer.append(data.copy())\n",
    "            else:\n",
    "                if speaking:\n",
    "                    buffer.append(data.copy())\n",
    "                    if silence_start is None:\n",
    "                        silence_start = now\n",
    "                    elif now - silence_start >= silence_duration:\n",
    "                        break\n",
    "            if now - start_time >= max_seconds:\n",
    "                break\n",
    "    if not buffer:\n",
    "        return None\n",
    "    return np.concatenate(buffer, axis=0)\n",
    "\n",
    "def numpy_to_wav_bytes(audio: np.ndarray, sample_rate: int) -> bytes:\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    int_audio = (audio * 32767).astype(np.int16)\n",
    "    with io.BytesIO() as output:\n",
    "        with wave.open(output, 'wb') as wf:\n",
    "            wf.setnchannels(1)\n",
    "            wf.setsampwidth(2)\n",
    "            wf.setframerate(sample_rate)\n",
    "            wf.writeframes(int_audio.tobytes())\n",
    "        return output.getvalue()\n",
    "\n",
    "class VoiceAssistantSession:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: OpenAI,\n",
    "        system_prompt: str,\n",
    "        sample_rate: int = 16000,\n",
    "        threshold: float = 0.01,\n",
    "        silence_duration: float = 1.0,\n",
    "        max_seconds: float = 30.0\n",
    "    ) -> None:\n",
    "        self.client = client\n",
    "        self.system_prompt = system_prompt\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = threshold\n",
    "        self.silence_duration = silence_duration\n",
    "        self.max_seconds = max_seconds\n",
    "        self.history: List[Dict[str, List[Dict[str, str]]]] = []\n",
    "        self.player = AudioPlayer()\n",
    "\n",
    "    def stop_playback(self) -> None:\n",
    "        self.player.stop()\n",
    "\n",
    "    def reset_history(self) -> None:\n",
    "        self.history.clear()\n",
    "        print('Conversation history cleared.')\n",
    "\n",
    "    def _build_messages(self) -> List[Dict[str, List[Dict[str, str]]]]:\n",
    "        return [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': [{ 'type': 'text', 'text': self.system_prompt }]\n",
    "            }\n",
    "        ] + self.history\n",
    "\n",
    "    def record_user(self) -> Optional[str]:\n",
    "        self.stop_playback()\n",
    "        print('Listening... start speaking, stay within microphone range.')\n",
    "        audio = record_until_silence(\n",
    "            sample_rate=self.sample_rate,\n",
    "            threshold=self.threshold,\n",
    "            silence_duration=self.silence_duration,\n",
    "            max_seconds=self.max_seconds\n",
    "        )\n",
    "        if audio is None:\n",
    "            print('No speech detected. Try again.')\n",
    "            return None\n",
    "        wav_bytes = numpy_to_wav_bytes(audio, self.sample_rate)\n",
    "        transcription = self.client.audio.transcriptions.create(\n",
    "            model='whisper-1',\n",
    "            file=('user.wav', wav_bytes, 'audio/wav')\n",
    "        )\n",
    "        user_text = transcription.text.strip()\n",
    "        if not user_text:\n",
    "            print('Transcription failed to capture speech.')\n",
    "            return None\n",
    "        print(f'You: {user_text}')\n",
    "        self.history.append({\n",
    "            'role': 'user',\n",
    "            'content': [{ 'type': 'text', 'text': user_text }]\n",
    "        })\n",
    "        return user_text\n",
    "\n",
    "    def respond(self) -> None:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model='gpt-4o-audio-preview',\n",
    "            modalities=['text', 'audio'],\n",
    "            audio={'voice': 'alloy', 'format': 'wav'},\n",
    "            messages=[msg for msg in self._build_messages()]\n",
    "        )\n",
    "        message = response.choices[0].message\n",
    "        assistant_text = message.content or ''\n",
    "        audio_bytes = None\n",
    "        if hasattr(message, 'audio') and message.audio:\n",
    "            audio_bytes = base64.b64decode(message.audio.data)\n",
    "        if assistant_text:\n",
    "            print(f'Assistant: {assistant_text}')\n",
    "            self.history.append({\n",
    "                'role': 'assistant',\n",
    "                'content': [{ 'type': 'text', 'text': assistant_text }]\n",
    "            })\n",
    "        if audio_bytes:\n",
    "            self.player.play_wav(audio_bytes)\n",
    "            # Wait for audio to finish playing\n",
    "            self.player.wait_finish()\n",
    "\n",
    "    def turn(self) -> None:\n",
    "        if self.record_user() is None:\n",
    "            return\n",
    "        self.respond()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579b451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice assistant session ready.\n"
     ]
    }
   ],
   "source": [
    "assistant_session = VoiceAssistantSession(\n",
    "    client=client,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    sample_rate=16000,\n",
    "    threshold=0.01,\n",
    "    silence_duration=1.0,\n",
    "    max_seconds=30.0\n",
    ")\n",
    "print('Voice assistant session ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d50868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter to speak (q to quit, r to reset history):  q\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'assistant_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m command = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPress Enter to speak (q to quit, r to reset history): \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m command.lower().strip() == \u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43massistant_session\u001b[49m.stop_playback()\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSession ended.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'assistant_session' is not defined"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    command = input(\"Press Enter to speak (q to quit, r to reset history): \")\n",
    "    if command.lower().strip() == 'q':\n",
    "        assistant_session.stop_playback()\n",
    "        print('Session ended.')\n",
    "        break\n",
    "    if command.lower().strip() == 'r':\n",
    "        assistant_session.reset_history()\n",
    "        continue\n",
    "    assistant_session.stop_playback()\n",
    "    assistant_session.turn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f61ab5-ce72-4f0b-a6ff-9352c5746d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
